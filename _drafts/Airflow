1. What is airflow
Airflow is an 
2. Quick run
3. Archtecture
Executor
Dag
Directed Acyclic Graph – is a workflow of all the tasks you want to run, organized in a way that reflects their relationships and dependencies In general, each one should correspond to a single logical workflow
Dagbag
- default dags
-- /usr/local/airflow/dags
Task
A work unit in a DAG, it is an implementation of an Operator. for example a PythonOperator to execute some Python code, or a BashOperator to run a Bash command
Task instance
A runtime exection of a task, It should be a combination of a DAG, a task, and a point in time (execution_date)
Job
UI
command line tools
webserver 
scheduler
Sqlite

Dag folder
A folder where airflow is searching DAG definition python files, execute and build DAG objects into airflow system. 
When searching for DAGs, Airflow only considers python files that contain the strings “airflow” and “DAG” by default. 
To consider all python files instead, disable the DAG_DISCOVERY_SAFE_MODE configuration flag.

Dag definition
Airflow use python file to declare dag objects, a DAG function is provided to accept configurations. e.g. dag=DAG('dag_name', default_args=default_args)
Dag arguments
The arguments provided for the dag object when the dag is run
Dag default arguments
The default dag argument, which is provided when defining the dag in dag definition python file

Context manager
airflow will find a dag definition in a python and assign that dat to operators defined in that definition file

Dag objects build
Airflow will find and run dag definition python files to create dags

Task relationship
Tasks a a dag can has dependencies by using task_1 >> task_2
Upstream
if we have a taskA, then the task running before it is the upstream of taskA, task A need to wait until upstream has completed successfully.
Downsteam
if we have a taskA, then the task after it is the downstream of taskA

Task lifecycle
A task goes through various stages from start to completion
https://airflow.apache.org/docs/stable/_images/task_lifecycle_diagram.png

Operators
Operators is specific implementations of tasks. It defines what the task does and are only loaded by Airflow if they are assigned to a DAG.
BashOperator - executes a bash command
PythonOperator - calls an arbitrary Python function
EmailOperator - sends an email
SimpleHttpOperator - sends an HTTP request

Bitshift Composition
A convinient way to define relationships between tasks.

Pool
A named of list of workers which can be refered by a task to manually balance airflow workload.

Queue
Queue for a executor to cache tasks

Worker
Processes to get tasks out from queue and run it, workers can listen to one or multiple queues of tasks. 


4. Basic usage


5. References

https://airflow.apache.org/docs/stable/cli.html
usage: airflow [-h]
               {backfill,list_dag_runs,list_tasks,clear,pause,unpause,trigger_dag,delete_dag,show_dag,pool,variables,kerberos,render,run,initdb,list_dags,dag_state,task_failed_deps,task_state,serve_logs,test,webserver,resetdb,upgradedb,checkdb,shell,scheduler,worker,flower,version,connections,create_user,delete_user,list_users,sync_perm,next_execution,rotate_fernet_key}
               ...

positional arguments:
  {backfill,list_dag_runs,list_tasks,clear,pause,unpause,trigger_dag,delete_dag,show_dag,pool,variables,kerberos,render,run,initdb,list_dags,dag_state,task_failed_deps,task_state,serve_logs,test,webserver,resetdb,upgradedb,checkdb,shell,scheduler,worker,flower,version,connections,create_user,delete_user,list_users,sync_perm,next_execution,rotate_fernet_key}
                        sub-command help
    backfill            Run subsections of a DAG for a specified date range.
                        If reset_dag_run option is used, backfill will first
                        prompt users whether airflow should clear all the
                        previous dag_run and task_instances within the
                        backfill date range. If rerun_failed_tasks is used,
                        backfill will auto re-run the previous failed task
                        instances within the backfill date range.
    list_dag_runs       List dag runs given a DAG id. If state option is
                        given, it will onlysearch for all the dagruns with the
                        given state. If no_backfill option is given, it will
                        filter outall backfill dagruns for given dag id.
    list_tasks          List the tasks within a DAG
    clear               Clear a set of task instance, as if they never ran
    pause               Pause a DAG
    unpause             Resume a paused DAG
    trigger_dag         Trigger a DAG run
    delete_dag          Delete all DB records related to the specified DAG
    show_dag            Displays DAG's tasks with their dependencies
    pool                CRUD operations on pools
    variables           CRUD operations on variables
    kerberos            Start a kerberos ticket renewer
    render              Render a task instance's template(s)
    run                 Run a single task instance
    initdb              Initialize the metadata database
    list_dags           List all the DAGs
    dag_state           Get the status of a dag run
    task_failed_deps    Returns the unmet dependencies for a task instance
                        from the perspective of the scheduler. In other words,
                        why a task instance doesn't get scheduled and then
                        queued by the scheduler, and then run by an executor).
    task_state          Get the status of a task instance
    serve_logs          Serve logs generate by worker
    test                Test a task instance. This will run a task without
                        checking for dependencies or recording its state in
                        the database.
    webserver           Start a Airflow webserver instance
    resetdb             Burn down and rebuild the metadata database
    upgradedb           Upgrade the metadata database to latest version
    checkdb             Check if the database can be reached.
    shell               Runs a shell to access the database
    scheduler           Start a scheduler instance
    worker              Start a Celery worker node
    flower              Start a Celery Flower
    version             Show the version
    connections         List/Add/Delete connections
    create_user         Create an account for the Web UI (FAB-based)
    delete_user         Delete an account for the Web UI
    list_users          List accounts for the Web UI
    sync_perm           Update permissions for existing roles and DAGs.
    next_execution      Get the next execution datetime of a DAG.
    rotate_fernet_key   Rotate all encrypted connection credentials and
                        variables; see
                        https://airflow.readthedocs.io/en/stable/howto/secure-
                        connections.html#rotating-encryption-keys.

optional arguments:
  -h, --help            show this help message and exit
